From da8cf6eba00597f6e4152395ec4b3fd882e8eae2 Mon Sep 17 00:00:00 2001
From: Dirreke <mingyang_ge@163.com>
Date: Thu, 21 Mar 2024 21:20:45 +0800
Subject: [PATCH 08/46] Update to RocksDB 9.0.0

---
 Cargo.toml                      |   4 +-
 librocksdb-sys/Cargo.toml       |   2 +-
 librocksdb-sys/build_version.cc |   8 +-
 librocksdb-sys/rocksdb          |   2 +-
 src/db_options.rs               |  22 ---
 src/statistics.rs               | 270 ++++++++++++++++----------------
 6 files changed, 142 insertions(+), 166 deletions(-)

--- rocksdb.orig/src/db_options.rs
+++ rocksdb/src/db_options.rs
@@ -2849,17 +2849,6 @@
         }
     }
 
-    /// Specifies the file access pattern once a compaction is started.
-    ///
-    /// It will be applied to all input files of a compaction.
-    ///
-    /// Default: Normal
-    pub fn set_access_hint_on_compaction_start(&mut self, pattern: AccessHint) {
-        unsafe {
-            ffi::rocksdb_options_set_access_hint_on_compaction_start(self.inner, pattern as c_int);
-        }
-    }
-
     /// Enable/disable adaptive mutex, which spins in the user space before resorting to kernel.
     ///
     /// This could reduce context switch when the mutex is not
@@ -3986,17 +3975,6 @@
     SkipAnyCorruptedRecord = ffi::rocksdb_skip_any_corrupted_records_recovery as isize,
 }
 
-/// File access pattern once a compaction has started
-#[derive(Debug, Copy, Clone, PartialEq, Eq)]
-#[cfg_attr(feature = "serde1", derive(serde::Serialize, serde::Deserialize))]
-#[repr(i32)]
-pub enum AccessHint {
-    None = 0,
-    Normal,
-    Sequential,
-    WillNeed,
-}
-
 pub struct FifoCompactOptions {
     pub(crate) inner: *mut ffi::rocksdb_fifo_compaction_options_t,
 }
--- rocksdb.orig/src/statistics.rs
+++ rocksdb/src/statistics.rs
@@ -146,6 +146,42 @@
         /// # of bytes written into cache.
         BlockCacheBytesWrite("rocksdb.block.cache.bytes.write"),
 
+        BlockCacheCompressionDictMiss("rocksdb.block.cache.compression.dict.miss"),
+        BlockCacheCompressionDictHit("rocksdb.block.cache.compression.dict.hit"),
+        BlockCacheCompressionDictAdd("rocksdb.block.cache.compression.dict.add"),
+        BlockCacheCompressionDictBytesInsert("rocksdb.block.cache.compression.dict.bytes.insert"),
+
+        /// # of blocks redundantly inserted into block cache.
+        /// REQUIRES: BlockCacheAddRedundant <= BlockCacheAdd
+        BlockCacheAddRedundant("rocksdb.block.cache.add.redundant"),
+        /// # of index blocks redundantly inserted into block cache.
+        /// REQUIRES: BlockCacheIndexAddRedundant <= BlockCacheIndexAdd
+        BlockCacheIndexAddRedundant("rocksdb.block.cache.index.add.redundant"),
+        /// # of filter blocks redundantly inserted into block cache.
+        /// REQUIRES: BlockCacheFilterAddRedundant <= BlockCacheFilterAdd
+        BlockCacheFilterAddRedundant("rocksdb.block.cache.filter.add.redundant"),
+        /// # of data blocks redundantly inserted into block cache.
+        /// REQUIRES: BlockCacheDataAddRedundant <= BlockCacheDataAdd
+        BlockCacheDataAddRedundant("rocksdb.block.cache.data.add.redundant"),
+        /// # of dict blocks redundantly inserted into block cache.
+        /// REQUIRES: BlockCacheCompressionDictAddRedundant
+        ///           <= BlockCacheCompressionDictAdd
+        BlockCacheCompressionDictAddRedundant("rocksdb.block.cache.compression.dict.add.redundant"),
+
+        /// Secondary cache statistics
+        SecondaryCacheHits("rocksdb.secondary.cache.hits"),
+
+        /// Fine grained secondary cache stats
+        SecondaryCacheFilterHits("rocksdb.secondary.cache.filter.hits"),
+        SecondaryCacheIndexHits("rocksdb.secondary.cache.index.hits"),
+        SecondaryCacheDataHits("rocksdb.secondary.cache.data.hits"),
+
+        /// Compressed secondary cache related stats
+        CompressedSecondaryCacheDummyHits("rocksdb.compressed.secondary.cache.dummy.hits"),
+        CompressedSecondaryCacheHits("rocksdb.compressed.secondary.cache.hits"),
+        CompressedSecondaryCachePromotions("rocksdb.compressed.secondary.cache.promotions"),
+        CompressedSecondaryCachePromotionSkips("rocksdb.compressed.secondary.cache.promotion.skips"),
+
         /// # of times bloom filter has avoided file reads, i.e., negatives.
         BloomFilterUseful("rocksdb.bloom.filter.useful"),
         /// # of times bloom FullFilter has not avoided the reads.
@@ -153,6 +189,16 @@
         /// # of times bloom FullFilter has not avoided the reads and data actually
         /// exist.
         BloomFilterFullTruePositive("rocksdb.bloom.filter.full.true.positive"),
+        /// Prefix filter stats when used for point lookups (Get / MultiGet).
+        /// (For prefix filter stats on iterators, see *_LEVEL_Seek_*.)
+        /// Checked: filter was queried
+        BloomFilterPrefixChecked("rocksdb.bloom.filter.prefix.checked"),
+        /// Useful: filter returned false so prevented accessing data+index blocks
+        BloomFilterPrefixUseful("rocksdb.bloom.filter.prefix.useful"),
+        /// True positive: found a key matching the point query. When another key
+        /// with the same prefix matches, it is considered a false positive by
+        /// these statistics even though the filter returned a true positive.
+        BloomFilterPrefixTruePositive("rocksdb.bloom.filter.prefix.true.positive"),
 
         /// # persistent cache hit
         PersistentCacheHit("rocksdb.persistent.cache.hit"),
@@ -176,21 +222,21 @@
         /// # of Get() queries served by L2 and up
         GetHitL2AndUp("rocksdb.l2andup.hit"),
 
-        /**
-         * Compaction_KeyDrop* count the reasons for key drop during compaction
-         * There are 4 reasons currently.
-         */
-        CompactionKeyDropNewerEntry("rocksdb.compaction.key.drop.new"),
+        ///
+        /// Compaction_KeyDrop* count the reasons for key drop during compaction
+        /// There are 4 reasons currently.
+        ///
         /// key was written with a newer value.
         /// Also includes keys dropped for range del.
-        CompactionKeyDropObsolete("rocksdb.compaction.key.drop.obsolete"),
+        CompactionKeyDropNewerEntry("rocksdb.compaction.key.drop.new"),
         /// The key is obsolete.
-        CompactionKeyDropRangeDel("rocksdb.compaction.key.drop.range_del"),
+        CompactionKeyDropObsolete("rocksdb.compaction.key.drop.obsolete"),
         /// key was covered by a range tombstone.
-        CompactionKeyDropUser("rocksdb.compaction.key.drop.user"),
+        CompactionKeyDropRangeDel("rocksdb.compaction.key.drop.range_del"),
         /// user compaction function has dropped the key.
-        CompactionRangeDelDropObsolete("rocksdb.compaction.range_del.drop.obsolete"),
+        CompactionKeyDropUser("rocksdb.compaction.key.drop.user"),
         /// all keys in range were deleted.
+        CompactionRangeDelDropObsolete("rocksdb.compaction.range_del.drop.obsolete"),
         /// Deletions obsoleted before bottom level due to file gap optimization.
         CompactionOptimizedDelDropObsolete("rocksdb.compaction.optimized.del.drop.obsolete"),
         /// If a compaction was canceled in sfm to prevent ENOSPC
@@ -221,6 +267,17 @@
         /// The number of uncompressed bytes read from an iterator.
         /// Includes size of key and value.
         IterBytesRead("rocksdb.db.iter.bytes.read"),
+        /// Number of internal keys skipped by Iterator
+        NumberIterSkip("rocksdb.number.iter.skip"),
+        /// Number of times we had to reseek inside an iteration to skip
+        /// over large number of keys with same userkey.
+        NumberOfReseeksInIteration("rocksdb.number.reseeks.iteration"),
+
+        /// number of iterators created
+        NoIteratorCreated("rocksdb.num.iterator.created"),
+        /// number of iterators deleted
+        NoIteratorDeleted("rocksdb.num.iterator.deleted"),
+
         NoFileOpens("rocksdb.no.file.opens"),
         NoFileErrors("rocksdb.no.file.errors"),
         /// Writer has to wait for compaction or flush to finish.
@@ -233,24 +290,13 @@
         NumberMultigetCalls("rocksdb.number.multiget.get"),
         NumberMultigetKeysRead("rocksdb.number.multiget.keys.read"),
         NumberMultigetBytesRead("rocksdb.number.multiget.bytes.read"),
+        /// Number of keys actually found in MultiGet calls (vs number requested by
+        /// caller)
+        /// NumberMultigetKeys_Read gives the number requested by caller
+        NumberMultigetKeysFound("rocksdb.number.multiget.keys.found"),
 
         NumberMergeFailures("rocksdb.number.merge.failures"),
 
-        /// Prefix filter stats when used for point lookups (Get / MultiGet).
-        /// (For prefix filter stats on iterators, see *_LEVEL_Seek_*.)
-        /// Checked: filter was queried
-        BloomFilterPrefixChecked("rocksdb.bloom.filter.prefix.checked"),
-        /// Useful: filter returned false so prevented accessing data+index blocks
-        BloomFilterPrefixUseful("rocksdb.bloom.filter.prefix.useful"),
-        /// True positive: found a key matching the point query. When another key
-        /// with the same prefix matches, it is considered a false positive by
-        /// these statistics even though the filter returned a true positive.
-        BloomFilterPrefixTruePositive("rocksdb.bloom.filter.prefix.true.positive"),
-
-        /// Number of times we had to reseek inside an iteration to skip
-        /// over large number of keys with same userkey.
-        NumberOfReseeksInIteration("rocksdb.number.reseeks.iteration"),
-
         /// Record the number of calls to GetUpdatesSince. Useful to keep track of
         /// transaction log iterator refreshes
         GetUpdatesSinceCalls("rocksdb.getupdatessince.calls"),
@@ -262,16 +308,17 @@
         /// Writes can be processed by requesting thread or by the thread at the
         /// head of the writers queue.
         WriteDoneBySelf("rocksdb.write.self"),
-        WriteDoneByOther("rocksdb.write.other"),
         /// Equivalent to writes done for others
-        WriteWithWal("rocksdb.write.wal"),
+        WriteDoneByOther("rocksdb.write.other"),
         /// Number of Write calls that request WAL
-        CompactReadBytes("rocksdb.compact.read.bytes"),
+        WriteWithWal("rocksdb.write.wal"),
         /// Bytes read during compaction
-        CompactWriteBytes("rocksdb.compact.write.bytes"),
+        CompactReadBytes("rocksdb.compact.read.bytes"),
         /// Bytes written during compaction
-        FlushWriteBytes("rocksdb.flush.write.bytes"),
+        CompactWriteBytes("rocksdb.compact.write.bytes"),
         /// Bytes written during flush
+        FlushWriteBytes("rocksdb.flush.write.bytes"),
+
 
         /// Compaction read and write statistics broken down by CompactionReason
         CompactReadBytesMarked("rocksdb.compact.read.marked.bytes"),
@@ -292,8 +339,35 @@
         NumberBlockCompressed("rocksdb.number.block.compressed"),
         NumberBlockDecompressed("rocksdb.number.block.decompressed"),
 
-        /// DEPRECATED / unused (see NumberBlockCompression_*)
-        NumberBlockNotCompressed("rocksdb.number.block.not_compressed"),
+        /// Number of input bytes (uncompressed) to compression for SST blocks that
+        /// are stored compressed.
+        BytesCompressedFrom("rocksdb.bytes.compressed.from"),
+        /// Number of output bytes (compressed) from compression for SST blocks that
+        /// are stored compressed.
+        BytesCompressedTo("rocksdb.bytes.compressed.to"),
+        /// Number of uncompressed bytes for SST blocks that are stored uncompressed
+        /// because compression type is kNoCompression, or some error case caused
+        /// compression not to run or produce an output. Index blocks are only counted
+        /// if enable_index_compression is true.
+        BytesCompressionBypassed("rocksdb.bytes.compression_bypassed"),
+        /// Number of input bytes (uncompressed) to compression for SST blocks that
+        /// are stored uncompressed because the compression result was rejected,
+        /// either because the ratio was not acceptable (see
+        /// CompressionOptions::max_compressed_bytes_per_kb) or found invalid by the
+        /// `verify_compression` option.
+        BytesCompressionRejected("rocksdb.bytes.compression.rejected"),
+
+        /// Like BytesCompressionBypassed but counting number of blocks
+        NumberBlockCompressionBypassed("rocksdb.number.block_compression_bypassed"),
+        /// Like BytesCompressionRejected but counting number of blocks
+        NumberBlockCompressionRejected("rocksdb.number.block_compression_rejected"),
+
+        /// Number of input bytes (compressed) to decompression in reading compressed
+        /// SST blocks from storage.
+        BytesDecompressedFrom("rocksdb.bytes.decompressed.from"),
+        /// Number of output bytes (uncompressed) from decompression in reading
+        /// compressed SST blocks from storage.
+        BytesDecompressedTo("rocksdb.bytes.decompressed.to"),
 
         /// Tickers that record cumulative time.
         MergeOperationTotalTime("rocksdb.merge.operation.time.nanos"),
@@ -309,17 +383,15 @@
         /// (ReadAMP_ToTAL_ReadBytes / Read_AMP_Estimate_UsefulBytes)
         //
         /// REQUIRES: ReadOptions::read_amp_bytes_per_bit to be enabled
-        ReadAmpEstimateUsefulBytes("rocksdb.read.amp.estimate.useful.bytes"),
         /// Estimate of total bytes actually used.
-        ReadAmpTotalReadBytes("rocksdb.read.amp.total.read.bytes"),
+        ReadAmpEstimateUsefulBytes("rocksdb.read.amp.estimate.useful.bytes"),
         /// Total size of loaded data blocks.
+        ReadAmpTotalReadBytes("rocksdb.read.amp.total.read.bytes"),
+
 
         /// Number of refill intervals where rate limiter's bytes are fully consumed.
         NumberRateLimiterDrains("rocksdb.number.rate_limiter.drains"),
 
-        /// Number of internal keys skipped by Iterator
-        NumberIterSkip("rocksdb.number.iter.skip"),
-
         /// BlobDB specific stats
         /// # of Put/PutTtl/PutUntil to BlobDB. Only applicable to legacy BlobDB.
         BlobDbNumPut("rocksdb.blobdb.num.put"),
@@ -398,6 +470,20 @@
         /// applicable to legacy BlobDB.
         BlobDbFifoBytesEvicted("rocksdb.blobdb.fifo.bytes.evicted"),
 
+        /// Integrated BlobDB specific stats
+        /// # of times cache miss when accessing blob from blob cache.
+        BlobDbCacheMiss("rocksdb.blobdb.cache.miss"),
+        /// # of times cache hit when accessing blob from blob cache.
+        BlobDbCacheHit("rocksdb.blobdb.cache.hit"),
+        /// # of data blocks added to blob cache.
+        BlobDbCacheAdd("rocksdb.blobdb.cache.add"),
+        /// # of failures when adding blobs to blob cache.
+        BlobDbCacheAddFailures("rocksdb.blobdb.cache.add.failures"),
+        /// # of bytes read from blob cache.
+        BlobDbCacheBytesRead("rocksdb.blobdb.cache.bytes.read"),
+        /// # of bytes written into blob cache.
+        BlobDbCacheBytesWrite("rocksdb.blobdb.cache.bytes.write"),
+
         /// These counters indicate a performance issue in WritePrepared transactions.
         /// We should not seem them ticking them much.
         /// # of times prepare_mutex_ is acquired in the fast path.
@@ -411,37 +497,6 @@
         /// # of times ::Get returned TryAgain due to expired snapshot seq
         TxnGetTryAgain("rocksdb.txn.get.tryagain"),
 
-        /// Number of keys actually found in MultiGet calls (vs number requested by
-        /// caller)
-        /// NumberMultigetKeys_Read gives the number requested by caller
-        NumberMultigetKeysFound("rocksdb.number.multiget.keys.found"),
-
-        NoIteratorCreated("rocksdb.num.iterator.created"),
-        /// number of iterators created
-        NoIteratorDeleted("rocksdb.num.iterator.deleted"),
-        /// number of iterators deleted
-        BlockCacheCompressionDictMiss("rocksdb.block.cache.compression.dict.miss"),
-        BlockCacheCompressionDictHit("rocksdb.block.cache.compression.dict.hit"),
-        BlockCacheCompressionDictAdd("rocksdb.block.cache.compression.dict.add"),
-        BlockCacheCompressionDictBytesInsert("rocksdb.block.cache.compression.dict.bytes.insert"),
-
-        /// # of blocks redundantly inserted into block cache.
-        /// REQUIRES: BlockCacheAddRedundant <= BlockCacheAdd
-        BlockCacheAddRedundant("rocksdb.block.cache.add.redundant"),
-        /// # of index blocks redundantly inserted into block cache.
-        /// REQUIRES: BlockCacheIndexAddRedundant <= BlockCacheIndexAdd
-        BlockCacheIndexAddRedundant("rocksdb.block.cache.index.add.redundant"),
-        /// # of filter blocks redundantly inserted into block cache.
-        /// REQUIRES: BlockCacheFilterAddRedundant <= BlockCacheFilterAdd
-        BlockCacheFilterAddRedundant("rocksdb.block.cache.filter.add.redundant"),
-        /// # of data blocks redundantly inserted into block cache.
-        /// REQUIRES: BlockCacheDataAddRedundant <= BlockCacheDataAdd
-        BlockCacheDataAddRedundant("rocksdb.block.cache.data.add.redundant"),
-        /// # of dict blocks redundantly inserted into block cache.
-        /// REQUIRES: BlockCacheCompressionDictAddRedundant
-        ///           <= BlockCacheCompressionDictAdd
-        BlockCacheCompressionDictAddRedundant("rocksdb.block.cache.compression.dict.add.redundant"),
-
         /// # of files marked as trash by sst file manager and will be deleted
         /// later by background thread.
         FilesMarkedTrash("rocksdb.files.marked.trash"),
@@ -453,14 +508,9 @@
 
         /// The counters for error handler, not that, bg_io_error is the subset of
         /// bg_error and bg_retryable_io_error is the subset of bg_io_error.
-        /// The misspelled versions are deprecated and only kept for compatibility.
-        /// ToDO: remove the misspelled tickers in the next major release.
         ErrorHandlerBgErrorCount("rocksdb.error.handler.bg.error.count"),
-        ErrorHandlerBgErrorCountMisspelled("rocksdb.error.handler.bg.errro.count"),
         ErrorHandlerBgIoErrorCount("rocksdb.error.handler.bg.io.error.count"),
-        ErrorHandlerBgIoErrorCountMisspelled("rocksdb.error.handler.bg.io.errro.count"),
         ErrorHandlerBgRetryableIoErrorCount("rocksdb.error.handler.bg.retryable.io.error.count"),
-        ErrorHandlerBgRetryableIoErrorCountMisspelled("rocksdb.error.handler.bg.retryable.io.errro.count"),
         ErrorHandlerAutoResumeCount("rocksdb.error.handler.autoresume.count"),
         ErrorHandlerAutoResumeRetryTotalCount("rocksdb.error.handler.autoresume.retry.total.count"),
         ErrorHandlerAutoResumeSuccessCount("rocksdb.error.handler.autoresume.success.count"),
@@ -471,9 +521,6 @@
         /// Outdated bytes of data present on memtable at flush time.
         MemtableGarbageBytesAtFlush("rocksdb.memtable.garbage.bytes.at.flush"),
 
-        /// Secondary cache statistics
-        SecondaryCacheHits("rocksdb.secondary.cache.hits"),
-
         /// Bytes read by `VerifyChecksum()` and `VerifyFileChecksums()` APIs.
         VerifyChecksumReadBytes("rocksdb.verify_checksum.read.bytes"),
 
@@ -534,30 +581,11 @@
 
         MultigetCoroutineCount("rocksdb.multiget.coroutine.count"),
 
-        /// Integrated BlobDB specific stats
-        /// # of times cache miss when accessing blob from blob cache.
-        BlobDbCacheMiss("rocksdb.blobdb.cache.miss"),
-        /// # of times cache hit when accessing blob from blob cache.
-        BlobDbCacheHit("rocksdb.blobdb.cache.hit"),
-        /// # of data blocks added to blob cache.
-        BlobDbCacheAdd("rocksdb.blobdb.cache.add"),
-        /// # of failures when adding blobs to blob cache.
-        BlobDbCacheAddFailures("rocksdb.blobdb.cache.add.failures"),
-        /// # of bytes read from blob cache.
-        BlobDbCacheBytesRead("rocksdb.blobdb.cache.bytes.read"),
-        /// # of bytes written into blob cache.
-        BlobDbCacheBytesWrite("rocksdb.blobdb.cache.bytes.write"),
-
         /// Time spent in the ReadAsync file system call
         ReadAsyncMicros("rocksdb.read.async.micros"),
         /// Number of errors returned to the async read callback
         AsyncReadErrorCount("rocksdb.async.read.error.count"),
 
-        /// Fine grained secondary cache stats
-        SecondaryCacheFilterHits("rocksdb.secondary.cache.filter.hits"),
-        SecondaryCacheIndexHits("rocksdb.secondary.cache.index.hits"),
-        SecondaryCacheDataHits("rocksdb.secondary.cache.data.hits"),
-
         /// Number of lookup into the prefetched tail (see
         /// `TableOpenPrefetchTailReadBytes`)
         /// that can't find its data for table open
@@ -573,36 +601,6 @@
         /// # of times timestamps can successfully help skip the table access
         TimestampFilterTableFiltered("rocksdb.timestamp.filter.table.filtered"),
 
-        /// Number of input bytes (uncompressed) to compression for SST blocks that
-        /// are stored compressed.
-        BytesCompressedFrom("rocksdb.bytes.compressed.from"),
-        /// Number of output bytes (compressed) from compression for SST blocks that
-        /// are stored compressed.
-        BytesCompressedTo("rocksdb.bytes.compressed.to"),
-        /// Number of uncompressed bytes for SST blocks that are stored uncompressed
-        /// because compression type is kNoCompression, or some error case caused
-        /// compression not to run or produce an output. Index blocks are only counted
-        /// if enable_index_compression is true.
-        BytesCompressionBypassed("rocksdb.bytes.compression_bypassed"),
-        /// Number of input bytes (uncompressed) to compression for SST blocks that
-        /// are stored uncompressed because the compression result was rejected,
-        /// either because the ratio was not acceptable (see
-        /// CompressionOptions::max_compressed_bytes_per_kb) or found invalid by the
-        /// `verify_compression` option.
-        BytesCompressionRejected("rocksdb.bytes.compression.rejected"),
-
-        /// Like BytesCompressionBypassed but counting number of blocks
-        NumberBlockCompressionBypassed("rocksdb.number.block_compression_bypassed"),
-        /// Like BytesCompressionRejected but counting number of blocks
-        NumberBlockCompressionRejected("rocksdb.number.block_compression_rejected"),
-
-        /// Number of input bytes (compressed) to decompression in reading compressed
-        /// SST blocks from storage.
-        BytesDecompressedFrom("rocksdb.bytes.decompressed.from"),
-        /// Number of output bytes (uncompressed) from decompression in reading
-        /// compressed SST blocks from storage.
-        BytesDecompressedTo("rocksdb.bytes.decompressed.to"),
-
         /// Number of times readahead is trimmed during scans when
         /// ReadOptions.auto_readahead_size is set.
         ReadAheadTrimmed("rocksdb.readahead.trimmed"),
@@ -619,12 +617,6 @@
 
         /// Number of FS reads avoided due to scan prefetching
         PrefetchHits("rocksdb.prefetch.hits"),
-
-        /// Compressed secondary cache related stats
-        CompressedSecondaryCacheDummyHits("rocksdb.compressed.secondary.cache.dummy.hits"),
-        CompressedSecondaryCacheHits("rocksdb.compressed.secondary.cache.hits"),
-        CompressedSecondaryCachePromotions("rocksdb.compressed.secondary.cache.promotions"),
-        CompressedSecondaryCachePromotionSkips("rocksdb.compressed.secondary.cache.promotion.skips"),
     }
 }
 
@@ -664,16 +656,22 @@
         FileReadDbIteratorMicros("rocksdb.file.read.db.iterator.micros"),
         FileReadVerifyDbChecksumMicros("rocksdb.file.read.verify.db.checksum.micros"),
         FileReadVerifyFileChecksumsMicros("rocksdb.file.read.verify.file.checksums.micros"),
+
+        // Time spent in writing SST files
+        SstWriteMicros("rocksdb.sst.write.micros"),
+        // Time spent in writing SST table (currently only block-based table) or blob
+        // file for flush, compaction or db open
+        FileWriteFlushMicros("rocksdb.file.write.flush.micros"),
+        FileWriteCompactionMicros("rocksdb.file.write.compaction.micros"),
+        FileWriteDbOpenMicros("rocksdb.file.write.db.open.micros"),
+
         /// The number of subcompactions actually scheduled during a compaction
         NumSubcompactionsScheduled("rocksdb.num.subcompactions.scheduled"),
         /// Value size distribution in each operation
         BytesPerRead("rocksdb.bytes.per.read"),
         BytesPerWrite("rocksdb.bytes.per.write"),
         BytesPerMultiget("rocksdb.bytes.per.multiget"),
-        BytesCompressed("rocksdb.bytes.compressed"),
-        /// DEPRECATED / unused (see BytesCompressed{From,To})
-        BytesDecompressed("rocksdb.bytes.decompressed"),
-        /// DEPRECATED / unused (see BytesDecompressed{From,To})
+
         CompressionTimesNanos("rocksdb.compression.times.nanos"),
         DecompressionTimesNanos("rocksdb.decompression.times.nanos"),
         /// Number of merge operands passed to the merge operator in user read
@@ -803,6 +801,6 @@
     assert_eq!(want, Ticker::BlockCacheIndexMiss.to_string());
 
     // assert enum lengths
-    assert_eq!(Ticker::iter().count(), 215 /* TICKER_ENUM_MAX */);
-    assert_eq!(Histogram::iter().count(), 60 /* HISTOGRAM_ENUM_MAX */);
+    assert_eq!(Ticker::iter().count(), 211 /* TICKER_ENUM_MAX */);
+    assert_eq!(Histogram::iter().count(), 62 /* HISTOGRAM_ENUM_MAX */);
 }
